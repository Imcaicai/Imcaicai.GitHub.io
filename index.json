[{"categories":["Tech"],"content":"本文为论文 Vision GNN: An Image is Worth Graph of Nodes 的阅读笔记。 论文下载：https://arxiv.org/abs/2206.00272 ","date":"2022-10-17","objectID":"/an-image-is-worth-graph-of-nodes/:0:0","tags":["深度学习"],"title":"Vision GNN: An Image is Worth Graph of Nodes","uri":"/an-image-is-worth-graph-of-nodes/"},{"categories":["Tech"],"content":"引言 网络架构在基于深度学习的计算机视觉中起着关键作用。广泛使用的CNN和 transformer（变换器）将图像视为 grid（网格）或 sequence（序列）结构，这对于捕捉不规则、复杂的物体来说是不灵活的。本文建议将图像表示为一个 graph 结构，并引入一个新的 Vision GNN（ViG）架构来提取视觉任务的图层特征。 文章主要工作： 介绍了计算机视觉方面的现有模型方法和成果 介绍ViG模型的构建过程及工作原理，为未来的研究提供有用的灵感和经验 通过图像分类和物体检测实验证明了ViG模型在视觉任务中的有效性 ","date":"2022-10-17","objectID":"/an-image-is-worth-graph-of-nodes/:1:0","tags":["深度学习"],"title":"Vision GNN: An Image is Worth Graph of Nodes","uri":"/an-image-is-worth-graph-of-nodes/"},{"categories":["Tech"],"content":"1 相关研究 CNN 曾经是计算机视觉中标准的网络结构，但近来 transformer with attention mechanism 、MLP-based 等模型也在不断发展，这些正在将视觉模型推向一个前所未有的高度。 ","date":"2022-10-17","objectID":"/an-image-is-worth-graph-of-nodes/:2:0","tags":["深度学习"],"title":"Vision GNN: An Image is Worth Graph of Nodes","uri":"/an-image-is-worth-graph-of-nodes/"},{"categories":["Tech"],"content":"1.1 3种图像结构 不同的网络结构以不同的方式处理输入的图像，通常有grid, sequence ,graph 3种，如下图所示。在 grid 和 sequence 结构中，节点只按空间位置排序；在 graph 结构中，节点是通过其内容连接的，不受局部位置的限制。 CNN 在图像上应用滑动窗口，并引入移位变异性和位置性；最近的 vision transformer 或 MLP 将图像视为 a sequence of patches（补丁序列）。 由于物体形状通常不是规则的四边形，常用的 grid 或 sequence 结构处理起图像来不够灵活，所以在本文中采用 graph 结构。 ","date":"2022-10-17","objectID":"/an-image-is-worth-graph-of-nodes/:2:1","tags":["深度学习"],"title":"Vision GNN: An Image is Worth Graph of Nodes","uri":"/an-image-is-worth-graph-of-nodes/"},{"categories":["Tech"],"content":"1.2 3种模型 CNN：曾经是计算机视觉中的主流网络结构，已经成功地应用于各种视觉任务，如图像分类、物体检测和语义分割。CNN模型在过去的十年里发展迅速，代表性的模型包括ResNet、MobileNet和NAS。 Vision transformer：从2020年开始，被引入到视觉任务中，ViT的一些变体开始被提出来以提高视觉任务的性能。主要的改进包括金字塔结，局部注意和位置编码。 MLP：通过专门设计的模块，MLP可以达到有竞争力的性能，并且在一般的视觉任务（如物体检测和分割）上工作。 ","date":"2022-10-17","objectID":"/an-image-is-worth-graph-of-nodes/:2:2","tags":["深度学习"],"title":"Vision GNN: An Image is Worth Graph of Nodes","uri":"/an-image-is-worth-graph-of-nodes/"},{"categories":["Tech"],"content":"1.3 GNN模型 1. GNN\u0026GCN GNN：图神经网络，由于传统的CNN网络无法表示顶点和边这种关系型数据，便出现了图神经网络解决这种图数据的表示问题，这属于CNN往图方向的应用扩展 GCN：图卷积神经网络，GNN在训练过程中，有将attention引入图结构的，有将门控机制引入图结构的，还有将卷积引入图结构的，引入卷积的GNN就是GCN，通过提取空间特征来进行学习 2. 发展 Micheli提出了早期的提出了早期的基于空间的GCN，Bruna等人提出了基于频谱的GCN，近几年来基于这两种GCN的改进和扩展也被提出。 3. 应用 GCN通常被应用于图数据，如社会网络、引文网络和生化图；在计算机视觉领域的应用主要包括点云分类、场景图生成和动作识别。 GCN只能解决自然形成的图的特定视觉任务，对于计算机视觉的一般应用，我们需要一个基于GCN的骨干网络来直接处理图像数据。 ","date":"2022-10-17","objectID":"/an-image-is-worth-graph-of-nodes/:2:3","tags":["深度学习"],"title":"Vision GNN: An Image is Worth Graph of Nodes","uri":"/an-image-is-worth-graph-of-nodes/"},{"categories":["Tech"],"content":"2 ViG模型 ","date":"2022-10-17","objectID":"/an-image-is-worth-graph-of-nodes/:3:0","tags":["深度学习"],"title":"Vision GNN: An Image is Worth Graph of Nodes","uri":"/an-image-is-worth-graph-of-nodes/"},{"categories":["Tech"],"content":"2.1 模型构建 Image→Graph 首先基于 graph 结构建立视觉图形神经网络，用于视觉任务。将输入的图像划分为若干个 patches（补丁），并将每个斑块视为一个 node （节点）。1 . 对于一个大小为 $H×W×3$ 的图像，我们将其分为 N 个补丁，把每个补丁转化为一个特征向量 $x_i∈R^D$，得到 $X = [x_1,x_2,…,x_N ]$， 其中 $D$ 是特征维度。将特征看做无序节点$V={v_1,v_2,…,v_N}$，节点$v_i$的k邻近节点记为$N(v_i)$，对每个$v_j∈N(v_i)$添加$v_j$到$v_i$的边$e_ji$。 最终得到图$G = (V,E) $，我们把图的构建过程记为$G = G(X)$。 图层处理 图卷积层可以通过聚合其邻居节点的特征在节点之间交换信息。具体操作方式为： $G' = F(G, W)=Update(Aggregate(G, W_agg), W_update) $ 其中，$W_agg$和 $W_update$是聚合、更新操作的可学习权重。 聚合：通过聚合邻居节点的特征来计算一个节点的表征 更新：进一步合并聚合的特征 通过最大相对卷积处理图层面，记为$X' = GraphConv(X)$。 $x_i' = h(x_i, g(x_i, N(x_i), W_agg), W_update)$ $g(·) = x_i'' = [x_i, max(${$x_j - x_i|j∈N(x_i)$}] $h(·) = x_i' = x_i'‘W_update$ . 接着进行图卷积的多头更新操作（有利于特征多样性），将聚合后的特征 $x_i’'$ 分割成 $h$ 个头，然后分别以不同的权重更新这些头，得到： $x_i' = [head^1W^1update, head^2W^2update,…head^hW^hupdate]$ ViG block ViG的2个基本模块 Graph模块：是在图卷积的基础上构建的，用于聚合和更新图形信息，可以缓解传统GNN的过度平滑现象 FFN模块：带有两个线性层，用于节点特征转换和鼓励节点多样性 以前的GCN通常重复使用卷积层来提取图形数据的聚合特征，这会导致过度平滑的现象 ，降低节点特征的显著性，如下图所示所示。为了解决这个问题，本文在ViG块中引入了更多的特征转换和非线性激活。 我们在图卷积前后应用一个线性层，将节点特征投射到同一领域，增加特征多样性。在图卷积之后插入一个非线性激活函数以避免层崩溃。我们称升级后的模块为Grapher模块，给定输入特征$X∈R^N×^D$ ，则可得到：$Y = σ(GraphConv(XW_in))W_out + X$ 2 . 其中$W_in$和$W_out$是全连接层的权重，σ是激活函数。为了进一步提高特征转换能力，我们在每个节点上利用前馈网络（FFN）：$Z = σ(YW_1)W_2 + Y$ 其中$W_1$和$W_2$是全连接层的权重。Graph模块和FFN模块的堆叠构成了ViG块，作为构建网络的基本单元。基于图像的graph结构和ViG块，我们可以建立ViG网络。 ","date":"2022-10-17","objectID":"/an-image-is-worth-graph-of-nodes/:3:1","tags":["深度学习"],"title":"Vision GNN: An Image is Worth Graph of Nodes","uri":"/an-image-is-worth-graph-of-nodes/"},{"categories":["Tech"],"content":"2.2 网络框架 各项同性结构：指主体在整个网络中具有同等大小和形状的特征 金字塔结构：考虑了图像的多尺度特性，提取特征的空间大小逐渐变小 在计算机视觉领域，常用的结构有各向同性结构和金字塔结构。为了与其他类型的神经网络有更普遍的比较，文章分别为ViG建立了这两种网络结构。 各向同性结构 文章建立了3个大小不同的各向同性ViG架构。为了扩大感受野，邻居结点的数量K从9线性增加到18；头的数量被设定为 h = 4。详情如下表：3 金字塔结构 文章建立了4个大小不同的金字塔ViG模型。详情如下：4 位置编码 为了表示节点的位置信息，文章为每个节点特征添加一个位置向量：$x_i←x_i+e_i$ ；金字塔结构中可以进一步使用相对位置编码。 ","date":"2022-10-17","objectID":"/an-image-is-worth-graph-of-nodes/:3:2","tags":["深度学习"],"title":"Vision GNN: An Image is Worth Graph of Nodes","uri":"/an-image-is-worth-graph-of-nodes/"},{"categories":["Tech"],"content":"2.3 模型优点 graph 是广义的数据结构，grid 和 sequence 可以看做 graph 的特例 graph 更灵活，可以对复杂、不规则的物体进行建模 一个物体可以被看作是由各个部分组成的，graph 结构可以构建他们的联系 ","date":"2022-10-17","objectID":"/an-image-is-worth-graph-of-nodes/:3:3","tags":["深度学习"],"title":"Vision GNN: An Image is Worth Graph of Nodes","uri":"/an-image-is-worth-graph-of-nodes/"},{"categories":["Tech"],"content":"3 实验 top1 accuracy：预测的label取最后概率向量里面最大的那一个作为预测结果，如果预测结果中概率最大的分类正确，则预测正确，否则预测错误。 top5 accuracy：最后概率向量最大的前五名中，只要出现了正确概率即为预测正确，否则预测错误。 ","date":"2022-10-17","objectID":"/an-image-is-worth-graph-of-nodes/:4:0","tags":["深度学习"],"title":"Vision GNN: An Image is Worth Graph of Nodes","uri":"/an-image-is-worth-graph-of-nodes/"},{"categories":["Tech"],"content":"3.1 实验结果 本文分别将各向同性结构、金字塔结构的ViG与同样结构的CNN、转化器和 MLPs对比，可以看出： 将图片视作Graph能够在计算机视觉任务中取得非常好的结果 和各向同性结构相比，金字塔结构的ViG具有更好的性能 各向同性结构的实验结果： 金字塔结构的实验结果： ","date":"2022-10-17","objectID":"/an-image-is-worth-graph-of-nodes/:4:1","tags":["深度学习"],"title":"Vision GNN: An Image is Worth Graph of Nodes","uri":"/an-image-is-worth-graph-of-nodes/"},{"categories":["Tech"],"content":"3.2 消融研究 消融研究：通过删除部分网络并研究网络的性能来更好的了解网络。 文章以各向同性的ViG-Ti为基础架构，在ImageNet分类任务上进行了消融研究，结果如下： 通过改变图卷积的类型，发现不同图卷积的Top-1准确率很高，说明ViG架构的灵活性。其中，最大相对卷积在计算量和精度之间实现了最佳的权衡。 直接利用图卷积进行图像分类的效果很差，可以通过添加更多的特征转换，如引入FC和FFN不断提高准确性。 太少的邻居结点会降低信息交流，太多会导致过度平滑。当邻居节点的数量在9-15的范围时表现较好。 头的数量 h=4时，计算量和精度可以最好平衡。 ","date":"2022-10-17","objectID":"/an-image-is-worth-graph-of-nodes/:4:2","tags":["深度学习"],"title":"Vision GNN: An Image is Worth Graph of Nodes","uri":"/an-image-is-worth-graph-of-nodes/"},{"categories":["Tech"],"content":"3.3 可视化 为了更好地理解本文的ViG模型是如何工作的，作者可视化了构建的图结构，展示了两个不同深度的样本的图。五角星是中心节点，相同颜色的节点是其邻居。 可以看到，在浅层，邻居节点往往是根据低层次、局部特征来选择的，如颜色和纹理；在深层层中，中心节点的邻居更具语义性，属于同一类别。而本文的ViG网络可以通过其内容和语义表征逐渐将节点联系起来，并帮助更好地识别物体。 参考资料： 从图(Graph)到图卷积(Graph Convolution)：漫谈图神经网络模型 (一) 图卷积神经网络(GCN) GRAPH CONVOLUTIONAL NETWORKS 不用像素当做节点的原因：会导致节点过多 ↩︎ 最后加上 $X$​ 是残差连接，为了避免过拟合。 ↩︎ FLOPs：浮点运算数，可以用来衡量算法/模型的复杂度。 ↩︎ E是FNN中的隐藏维度 ↩︎ ","date":"2022-10-17","objectID":"/an-image-is-worth-graph-of-nodes/:4:3","tags":["深度学习"],"title":"Vision GNN: An Image is Worth Graph of Nodes","uri":"/an-image-is-worth-graph-of-nodes/"},{"categories":["Tech"],"content":"1 结构 go run helloworld.go：执行Go代码 go build helloworld.go：编译生成二进制文件 ./helloworld：运行 import 声明必须跟在文件的 package 声明之后 Go 语言不需要在语句或者声明的末尾添加分号，除非一行上有多条语句 函数的左括号 { 必须和 func 函数声明在同一行上，且位于末尾，不能独占一行 在表达式 x+y 中，可在 + 后换行，不能在 + 前换行 2 基础语法 //格式化字符串 var stockcode = 123 var enddate = \"2020-12-31\" var url = \"Code=%d\u0026endDate=%s\" var target_url = fmt.Sprintf(url, stockcode, enddate) fmt.Println(target_url) 3 语言类型 布尔型 数字型 整形：int uint 浮点型：float complex 字符串 派生类型 4 变量 变量声明 var identifier type（指定变量类型，如果没有初始化，则变量默认为零值 var v_name = value（根据值自行判断变量类型 v_name := value（只能在函数体中出现 // 这种因式分解关键字的写法一般用于声明全局变量 var ( vname1 v_type1 vname2 v_type2 ) 局部变量不允许声明但不使用，全局变量可以 a, b = b, a （简单交换2个变量 _：空白标识符，也用于被抛弃值 5 常量 const identifier [type] = value //用作枚举 const ( Unknown = 0 Female = 1 Male = 2 ) 常量可以用len(), cap(), unsafe.Sizeof()函数计算表达式的值（必须是内置函数 iota 在const关键字出现时将被重置为0，const中每新增一行常量声明将使 iota 计数一次 6 条件语句 switch 匹配项后面也不需要再加 break fallthrough 执行后面的case case后面是类型不被局限于常量或整数，可以加多个，必须类型相同 func main() { var grade string = \"B\" var marks int = 90 switch marks { case 90: grade = \"A\" case 80,70: grade = \"B\" default: grade = \"D\" } switch { case grade == \"A\": fmt.Println(\"youxiu\") case grade == \"B\", grade == \"C\": fmt.Println(\"lianghao\") default: fmt.Println(\"cha\") } } type switch 判断某个 interface 变量中实际存储的变量类型 select 通信的 switch 语句 7 循环语句 for循环 for init; condition; post { } for condition { } for { } //range格式可以对 slice、map、数组、字符串等进行迭代循环 for key, value := range oldMap { newMap[key] = value } for key := range oldMap for _, value := range oldMap 在多重循环中，可以用标号 label 标出想 break 的循环 在多重循环中，可以用标号 label 标出想 continue 的循环 goto 语句可以无条件地转移到过程中指定的行 8 函数 func function_name( [parameter list] ) [return_types] { 函数体 } 函数可作为实参 匿名函数，可作为闭包 //方法 func (variable_name variable_data_type) function_name() [return_type]{ /* 函数体*/ } 9 变量作用域 局部变量：作用域只在函数体内 全局变量：整个包甚至外部包（被导出后）使用 全局变量与局部变量名称可以相同，但是函数内的局部变量会被优先考虑 10 数组 var variable_name [SIZE] variable_type 可以使用 ... 代替数组的长度 // 将索引为 1 和 3 的元素初始化 balance := [...]float32{1:2.0,3:7.0} 多维数组 使用 append() 函数向空的二维数组添加两行一维数组 //多维数组 var variable_name [SIZE1][SIZE2]...[SIZEN] variable_type 可以创建各个维度元素数量不一致的多维数组 //向函数传递数组 void myFunction(param [10]int) { ... } 11 指针 var var_name *var-type 指针数组 var ptr [MAX]*int; 指向指针的指针 var ptr **int; 12 结构体 //定义结构体 type struct_variable_type struct { member definition ... member definition } //声明变量 variable_name := structure_variable_type {value1, value2...valuen} variable_name := structure_variable_type { key1: value1, key2: value2..., keyn: valuen} 访问结构体：结构体.成员名 结构体作为函数参数 结构体指针 //声明 var struct_pointer *Books 结构体指针用 . 访问结构体成员 13 切片(Slice) var identifier []type //定义切片 var slice1 []type = make([]type, len) //创建切片 make([]T, length, capacity) //capacity指定容量，为可选参数 s :=[] int {1,2,3 } //直接初始化切片 s := arr[:] //初始化切片 s，是数组 arr 的引用 s := arr[startIndex:endIndex] //将 arr 中从下标 startIndex 到 endIndex-1 下的元素创建为一个新的切片 len() 方法获取长度 cap() 可以测量切片最长可以达到多少 空切片(nil)：切片未初始化，默认为nil，长度为0 copy() 方法拷贝切片 append() 方法向切片追加新元素 14 范围(range) 关键字用于 for 循环中迭代数组(array)、切片(slice)、通道(channel)或集合(map)的元素 在数组和切片中它返回元素的索引和索引对应的值，在集合中返回 key-value 对 //读取key,value for key, value := range oldMap { newMap[key] = value } //只读取key for key := range oldMap //只读取value for _, value := range oldMap range也可以用来枚举 Unicode 字符串 15 集合(Map) 无序的键值对的集合 /* 声明变量，默认 map 是 nil */ var map_variable map[key_data_type]value_data_type /* 使用 make 函数 */ map_variable := make(map[key_data_type]value_data_type) delete() 函数用于删除集合的元素, 参数为 map 和其对应的 key 16 接口 /* 定义接口 */ type interface_name interface { method_name1 [return_type] method_name2 [return_type] ... method_namen [return_type] } /* 定义结构体 */ type struct_name struct { /* variables */ } /* 实现接口方法 */ func (struct_name_variable struct_name) method_name1() [return_type] { /* 方法实现 */ } ... func (struct_name_variable struct_name) method_namen() [return_type] { /* 方法实现*/ } 17 错误处理 type error interface { Error() string } func Sqrt(f float64) (float64, error) { if f \u003c 0 { return 0, errors.New(\"math: square root of negative number\") } // 实现 } 18 并发 //goroutine语法 go 函数名( 参数列表 ) 同一个程序","date":"2022-10-16","objectID":"/go/:0:0","tags":["Go"],"title":"Go基本语法","uri":"/go/"}]